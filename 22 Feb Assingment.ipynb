{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db681eb",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6582c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Video 1 :\n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=moVLwr2XByQ\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video elements on the page\n",
    "    video_elements = soup.find_all('video')\n",
    "\n",
    "    # Extract the video URLs of the first five videos\n",
    "    video_urls = [video['src'] for video in video_elements[:5]]\n",
    "\n",
    "    # Print the video URLs\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {i}: {video_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6777b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 2 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=3Lb7wcQZauA\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video elements on the page\n",
    "    video_elements = soup.find_all('video')\n",
    "\n",
    "    # Extract the video URLs of the first five videos\n",
    "    video_urls = [video['src'] for video in video_elements[:5]]\n",
    "\n",
    "    # Print the video URLs\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {i}: {video_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6384536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 3 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video elements on the page\n",
    "    video_elements = soup.find_all('video')\n",
    "\n",
    "    # Extract the video URLs of the first five videos\n",
    "    video_urls = [video['src'] for video in video_elements[:5]]\n",
    "\n",
    "    # Print the video URLs\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {i}: {video_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca1eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 4 :\n",
    "    \n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video elements on the page\n",
    "    video_elements = soup.find_all('video')\n",
    "\n",
    "    # Extract the video URLs of the first five videos\n",
    "    video_urls = [video['src'] for video in video_elements[:5]]\n",
    "\n",
    "    # Print the video URLs\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {i}: {video_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8365df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 5 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=tUP7RQF5Pzg\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video elements on the page\n",
    "    video_elements = soup.find_all('video')\n",
    "\n",
    "    # Extract the video URLs of the first five videos\n",
    "    video_urls = [video['src'] for video in video_elements[:5]]\n",
    "\n",
    "    # Print the video URLs\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"Video {i}: {video_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca61be",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331790f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 1 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=moVLwr2XByQ\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video thumbnail elements on the page\n",
    "    thumbnail_elements = soup.find_all('img', class_='video-thumbnail')\n",
    "\n",
    "    # Extract the thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:5]]\n",
    "\n",
    "    # Print the thumbnail URLs\n",
    "    for i, thumbnail_url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Thumbnail {i}: {thumbnail_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d3a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 2 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=3Lb7wcQZauA\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video thumbnail elements on the page\n",
    "    thumbnail_elements = soup.find_all('img', class_='video-thumbnail')\n",
    "\n",
    "    # Extract the thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:5]]\n",
    "\n",
    "    # Print the thumbnail URLs\n",
    "    for i, thumbnail_url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Thumbnail {i}: {thumbnail_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c4b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 3 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video thumbnail elements on the page\n",
    "    thumbnail_elements = soup.find_all('img', class_='video-thumbnail')\n",
    "\n",
    "    # Extract the thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:5]]\n",
    "\n",
    "    # Print the thumbnail URLs\n",
    "    for i, thumbnail_url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Thumbnail {i}: {thumbnail_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280e0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 4 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video thumbnail elements on the page\n",
    "    thumbnail_elements = soup.find_all('img', class_='video-thumbnail')\n",
    "\n",
    "    # Extract the thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:5]]\n",
    "\n",
    "    # Print the thumbnail URLs\n",
    "    for i, thumbnail_url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Thumbnail {i}: {thumbnail_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8751211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 5 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=tUP7RQF5Pzg\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video thumbnail elements on the page\n",
    "    thumbnail_elements = soup.find_all('img', class_='video-thumbnail')\n",
    "\n",
    "    # Extract the thumbnail URLs of the first five videos\n",
    "    thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:5]]\n",
    "\n",
    "    # Print the thumbnail URLs\n",
    "    for i, thumbnail_url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"Thumbnail {i}: {thumbnail_url}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8882816",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2e89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 1 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=moVLwr2XByQ\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video title elements on the page\n",
    "    title_elements = soup.find_all('h2', class_='video-title')\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = [title.text.strip() for title in title_elements[:5]]\n",
    "\n",
    "    # Print the video titles\n",
    "    for i, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {i} Title: {title}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "094bdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 2 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=3Lb7wcQZauA\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video title elements on the page\n",
    "    title_elements = soup.find_all('h2', class_='video-title')\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = [title.text.strip() for title in title_elements[:5]]\n",
    "\n",
    "    # Print the video titles\n",
    "    for i, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {i} Title: {title}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf65b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 3 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video title elements on the page\n",
    "    title_elements = soup.find_all('h2', class_='video-title')\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = [title.text.strip() for title in title_elements[:5]]\n",
    "\n",
    "    # Print the video titles\n",
    "    for i, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {i} Title: {title}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc7fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 4 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video title elements on the page\n",
    "    title_elements = soup.find_all('h2', class_='video-title')\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = [title.text.strip() for title in title_elements[:5]]\n",
    "\n",
    "    # Print the video titles\n",
    "    for i, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {i} Title: {title}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257fafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 5 :\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.youtube.com/watch?v=tUP7RQF5Pzg\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all video title elements on the page\n",
    "    title_elements = soup.find_all('h2', class_='video-title')\n",
    "\n",
    "    # Extract the titles of the first five videos\n",
    "    video_titles = [title.text.strip() for title in title_elements[:5]]\n",
    "\n",
    "    # Print the video titles\n",
    "    for i, title in enumerate(video_titles, start=1):\n",
    "        print(f\"Video {i} Title: {title}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c13ff3",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25675e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=moVLwr2XByQ\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the views from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        views_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and views_element:\n",
    "            title = title_element.text.strip()\n",
    "            views = views_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Views: {views}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a763a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=3Lb7wcQZauA\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the views from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        views_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and views_element:\n",
    "            title = title_element.text.strip()\n",
    "            views = views_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Views: {views}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39174f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the views from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        views_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and views_element:\n",
    "            title = title_element.text.strip()\n",
    "            views = views_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Views: {views}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1338fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the views from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        views_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and views_element:\n",
    "            title = title_element.text.strip()\n",
    "            views = views_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Views: {views}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=tUP7RQF5Pzg\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the views from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        views_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and views_element:\n",
    "            title = title_element.text.strip()\n",
    "            views = views_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Views: {views}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d65941",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL of the YouTube page\n",
    "url =  \"https://www.youtube.com/watch?v=moVLwr2XByQ\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the time of posting from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        time_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and time_element:\n",
    "            title = title_element.text.strip()\n",
    "            time = time_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Time of Posting: {time}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=3Lb7wcQZauA\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the time of posting from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        time_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and time_element:\n",
    "            title = title_element.text.strip()\n",
    "            time = time_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Time of Posting: {time}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87815cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the time of posting from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        time_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and time_element:\n",
    "            title = title_element.text.strip()\n",
    "            time = time_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Time of Posting: {time}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06238650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=dbltT8mtIYk\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the time of posting from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        time_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and time_element:\n",
    "            title = title_element.text.strip()\n",
    "            time = time_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Time of Posting: {time}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL of the YouTube page\n",
    "url = \"https://www.youtube.com/watch?v=tUP7RQF5Pzg\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements that contain video information (usually contained in a class like \"style-scope ytd-rich-grid-media\")\n",
    "    video_elements = soup.find_all(class_='style-scope ytd-rich-grid-media')\n",
    "    \n",
    "    # Extract the time of posting from the first five videos\n",
    "    for i, video in enumerate(video_elements[:5]):\n",
    "        title_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        time_element = video.find(class_='style-scope ytd-rich-grid-media')\n",
    "        \n",
    "        if title_element and time_element:\n",
    "            title = title_element.text.strip()\n",
    "            time = time_element.text.strip()\n",
    "            print(f\"Video {i + 1}: {title} - Time of Posting: {time}\")\n",
    "        else:\n",
    "            print(f\"Video {i + 1}: Video information not found\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
