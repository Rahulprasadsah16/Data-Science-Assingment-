{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2138003",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba753a",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "\n",
    "In simple linear regression, there is only one independent variable that is used to predict the dependent variable.\n",
    "The relationship between the independent variable and the dependent variable is assumed to be linear, meaning it can be represented by a straight line.\n",
    "\n",
    "Example: Predicting house prices based on the square footage of the house. Here, square footage is the independent variable, and house price is the dependent variable\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3121ca",
   "metadata": {},
   "source": [
    "Multiple Linear Regression:\n",
    "\n",
    "In multiple linear regression, there are multiple independent variables that are used to predict the dependent variable.\n",
    "The relationship between the independent variables and the dependent variable is assumed to be linear.\n",
    "\n",
    "Example: Predicting a student's exam score based on study hours, previous exam scores, and attendance. Here, study hours, previous exam scores, and attendance are independent variables, and exam score is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865c564",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04074609",
   "metadata": {},
   "source": [
    "Linear regression makes several key assumptions:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means the change in the dependent variable is proportional to the change in the independent variable(s). You can check this assumption by plotting the independent variable(s) against the dependent variable and visually inspecting whether the relationship appears to be linear.\n",
    "\n",
    "Independence of errors: The errors (residuals) should be independent of each other. This means there should be no pattern in the residuals when plotted against the independent variables or the predicted values. You can check this assumption by examining residual plots or performing statistical tests such as the Durbin-Watson test for autocorrelation.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. In other words, the spread of the residuals should be the same across the range of predicted values. You can check this assumption by plotting the residuals against the predicted values and looking for a constant spread of points.\n",
    "\n",
    "Normality of errors: The errors should be normally distributed. This assumption is not necessary for the estimation of coefficients, but it is important for making valid statistical inferences and constructing confidence intervals. You can check this assumption by examining a histogram or a Q-Q plot of the residuals, or by conducting a formal statistical test such as the Shapiro-Wilk test.\n",
    "\n",
    "No perfect multicollinearity: There should be no exact linear relationship between the independent variables. In other words, one independent variable should not be a perfect predictor of another. You can check this assumption by calculating the variance inflation factor (VIF) for each independent variable, where VIF values greater than 10 suggest multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910430c",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe4c95",
   "metadata": {},
   "source": [
    "Let's consider a real-world scenario to illustrate this:\n",
    "\n",
    "Scenario: Suppose you are a real estate agent analyzing the relationship between the size of houses (in square feet) and their selling prices (in dollars). You collect data on various houses sold in a particular neighborhood.\n",
    "\n",
    "Linear Regression Model: You decide to build a linear regression model to predict the selling price of a house based on its size. Your model equation is:\n",
    "\n",
    "Selling Price\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "×\n",
    "Size of House\n",
    "+\n",
    "𝜖\n",
    "Selling Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Size of House+ϵ\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Slope (\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): Let's say the estimated slope coefficient (\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ) is 100. This means that for every one-unit increase in the size of the house (e.g., going from 1500 square feet to 1501 square feet), the selling price is expected to increase by $100, holding all other factors constant. So, larger houses tend to sell for higher prices, and the rate of increase in price for an additional square foot is $100.\n",
    "\n",
    "Intercept (\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): Suppose the estimated intercept (\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ) is $50,000. This means that for a house with zero square feet (which is nonsensical in reality), the predicted selling price would be $50,000 according to the model. However, in this scenario, the intercept might not have a practical interpretation because houses with zero square feet do not exist. It's just the point where the regression line intersects the y-axis.\n",
    "\n",
    "In summary, the slope of the regression line tells us the rate of change in the dependent variable for a one-unit change in the independent variable, while the intercept represents the predicted value of the dependent variable when the independent variable is zero (if such a scenario is meaningful)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb76884",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3522b",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the loss function of a model by iteratively adjusting the parameters (weights and biases) of the model in the direction of steepest descent of the gradient. It is a first-order optimization algorithm commonly used in machine learning for training models, especially in the context of deep learning.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: The algorithm starts by initializing the parameters of the model with some arbitrary values. These parameters could be the weights and biases of the neurons in a neural network, for example.\n",
    "\n",
    "Compute Gradient: The gradient of the loss function with respect to each parameter is computed. The gradient represents the direction of the steepest ascent of the loss function. It tells us how much the loss function would change if we were to make a small change in each parameter.\n",
    "\n",
    "Update Parameters: The parameters are updated in the opposite direction of the gradient. This means subtracting a fraction of the gradient from each parameter. The fraction is determined by a parameter called the learning rate, which controls the size of the steps taken during optimization. The update rule for each parameter \n",
    "𝜃\n",
    "θ is typically:\n",
    "\n",
    "𝜃\n",
    "new\n",
    "=\n",
    "𝜃\n",
    "old\n",
    "−\n",
    "learning_rate\n",
    "×\n",
    "gradient\n",
    "θ \n",
    "new\n",
    "​\n",
    " =θ \n",
    "old\n",
    "​\n",
    " −learning_rate×gradient\n",
    "\n",
    "Iterate: Steps 2 and 3 are repeated iteratively until a stopping criterion is met, such as reaching a maximum number of iterations or achieving a sufficiently low value of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5a95d",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e54f8",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and multiple independent variables. While simple linear regression involves only one independent variable, multiple linear regression can handle two or more independent variables.\n",
    "\n",
    "The multiple linear regression model can be represented by the following equation:\n",
    "y\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑋\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "𝛽\n",
    "𝑝\n",
    "𝑋\n",
    "𝑝\n",
    "+\n",
    "𝜖\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "p\n",
    "​\n",
    " X \n",
    "p\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "Where:\n",
    "\n",
    "𝑌\n",
    "Y is the dependent variable.\n",
    "𝑋\n",
    "1\n",
    ",\n",
    "𝑋\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "𝑋\n",
    "𝑝\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "p\n",
    "​\n",
    "  are the independent variables.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept (constant term).\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "𝛽\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "𝛽\n",
    "𝑝\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "p\n",
    "​\n",
    "  are the coefficients of the independent variables, representing the change in \n",
    "𝑌\n",
    "Y for a one-unit change in each independent variable, holding all other variables constant.\n",
    "𝜖\n",
    "ϵ is the error term, representing the random variation in the dependent variable that is not explained by the independent variables.\n",
    "The key differences between multiple linear regression and simple linear regression are:\n",
    "\n",
    "Number of Independent Variables: In simple linear regression, there is only one independent variable, whereas in multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "Model Complexity: Multiple linear regression models are more complex than simple linear regression models because they involve multiple predictors. This complexity allows for capturing more nuanced relationships between the dependent and independent variables.\n",
    "\n",
    "Interpretation of Coefficients: In simple linear regression, the coefficient represents the change in the dependent variable for a one-unit change in the single independent variable. In multiple linear regression, each coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant. This allows for assessing the unique contribution of each independent variable to the dependent variable.\n",
    "\n",
    "Assumptions and Diagnostics: The assumptions and diagnostic tests for multiple linear regression are similar to those for simple linear regression but are extended to accommodate multiple predictors. These include assumptions about linearity, independence of errors, homoscedasticity, and normality of errors. Diagnostic tools such as residual plots and multicollinearity tests become more important in multiple linear regression due to the increased complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307a7a4",
   "metadata": {},
   "source": [
    "# 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693ad2",
   "metadata": {},
   "source": [
    "Multicollinearity occurs in multiple linear regression when two or more independent variables in the model are highly correlated with each other. This high correlation can cause issues with the estimation of the regression coefficients and lead to unreliable results.\n",
    "\n",
    "Here's how multicollinearity can manifest and its potential consequences:\n",
    "\n",
    "High Correlation Among Predictors: Multicollinearity is present when there is a high correlation (positive or negative) between two or more independent variables. This means that one predictor variable can be approximately linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "Unstable Coefficients: Multicollinearity can cause the estimated coefficients to be unstable or have large standard errors. This instability makes it difficult to determine the true relationship between each independent variable and the dependent variable.\n",
    "\n",
    "Difficulty in Interpretation: When multicollinearity is present, it becomes challenging to interpret the individual effects of each independent variable on the dependent variable. This is because the effect of one variable may be confounded with the effects of the correlated variables.\n",
    "\n",
    "Detecting and addressing multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation matrix among the independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficients is inflated due to multicollinearity. A VIF greater than 10 or 5 is often considered indicative of multicollinearity.\n",
    "\n",
    "Eigenvalues: Calculate the eigenvalues of the correlation matrix or the condition number of the design matrix. Large eigenvalues or condition numbers indicate multicollinearity.\n",
    "\n",
    "Inspect Scatterplots: Plot scatterplots between pairs of independent variables to visually inspect their relationships. High correlation might be evident from these plots.\n",
    "\n",
    "Addressing multicollinearity:\n",
    "\n",
    "Remove Redundant Variables: If two or more independent variables are highly correlated, consider removing one of them from the model to reduce multicollinearity.\n",
    "\n",
    "Feature Selection: Use techniques like stepwise regression or regularization methods (e.g., Lasso or Ridge regression) to select a subset of independent variables that provide the most predictive power while reducing multicollinearity.\n",
    "\n",
    "Combine Variables: Create new independent variables by combining or transforming existing variables to reduce multicollinearity.\n",
    "\n",
    "Principal Component Analysis (PCA): Use PCA to transform the original variables into a smaller set of uncorrelated principal components, which can then be used as predictors in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5142d",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaea6c",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable \n",
    "𝑋\n",
    "X and the dependent variable \n",
    "𝑌\n",
    "Y is modeled as an \n",
    "𝑛\n",
    "n-degree polynomial function. This means that instead of fitting a straight line to the data (as in linear regression), polynomial regression fits a curved line (a polynomial) to better capture non-linear relationships between variables.\n",
    "\n",
    "The polynomial regression model can be represented by the following equation:\n",
    "\n",
    "𝑌\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑋\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑋\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    " +ϵ\n",
    "\n",
    "Where:\n",
    "\n",
    "𝑌\n",
    "Y is the dependent variable.\n",
    "𝑋\n",
    "X is the independent variable.\n",
    "𝛽\n",
    "0\n",
    ",\n",
    "𝛽\n",
    "1\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "𝛽\n",
    "𝑛\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  are the coefficients of the polynomial terms.\n",
    "𝜖\n",
    "ϵ is the error term.\n",
    "The key differences between polynomial regression and linear regression are:\n",
    "\n",
    "Functional Form: In linear regression, the relationship between the dependent and independent variables is assumed to be linear, meaning the relationship can be represented by a straight line. In polynomial regression, the relationship can be non-linear, allowing for curved or polynomial-shaped relationships between variables.\n",
    "\n",
    "Flexibility: Polynomial regression is more flexible than linear regression because it can capture more complex relationships between variables. By fitting higher-degree polynomial terms (e.g., quadratic, cubic, etc.), polynomial regression can model curved or non-linear patterns in the data.\n",
    "\n",
    "Model Complexity: Polynomial regression models can become more complex as the degree of the polynomial increases. While linear regression involves estimating only two parameters (slope and intercept), polynomial regression involves estimating multiple coefficients for each polynomial term, making the model more complex and potentially prone to overfitting if not regularized properly.\n",
    "\n",
    "Interpretation: In linear regression, the interpretation of coefficients is straightforward: the slope coefficient represents the change in the dependent variable for a one-unit change in the independent variable. In polynomial regression, the interpretation becomes more complex as higher-degree polynomial terms are introduced. For example, the coefficient of the quadratic term (\n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    " ) represents the change in the rate of change of \n",
    "𝑌\n",
    "Y with respect to \n",
    "𝑋\n",
    "X, and so on for higher-degree terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28897e8",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81a049",
   "metadata": {},
   "source": [
    "Polynomial regression offers both advantages and disadvantages compared to linear regression, and the choice between the two depends on the nature of the data and the underlying relationship between variables. Here are some advantages and disadvantages of polynomial regression:\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture non-linear relationships between variables more effectively than linear regression. By fitting higher-degree polynomial terms, it can accommodate curved or complex patterns in the data.\n",
    "\n",
    "Improved Fit: In cases where the relationship between variables is non-linear, polynomial regression can provide a better fit to the data compared to linear regression. This can result in more accurate predictions and better model performance.\n",
    "\n",
    "Increased Model Complexity: Polynomial regression allows for modeling more complex relationships between variables by introducing higher-degree polynomial terms. This added complexity can lead to improved model performance, especially when the underlying relationship is highly non-linear.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Polynomial regression models with high-degree polynomial terms can be prone to overfitting, especially when the degree of the polynomial is too high relative to the size of the dataset. Overfitting occurs when the model captures noise or random fluctuations in the data instead of the underlying relationship.\n",
    "\n",
    "Interpretability: As the degree of the polynomial increases, the interpretation of coefficients becomes more complex. Higher-degree polynomial terms represent interactions and non-linear effects between variables, which can be challenging to interpret intuitively.\n",
    "\n",
    "Computational Complexity: Polynomial regression models with high-degree polynomial terms can be computationally intensive to estimate, especially when using iterative optimization algorithms. This can lead to longer training times and increased computational resources.\n",
    "\n",
    "Situation for Using Polynomial Regression:\n",
    "\n",
    "Polynomial regression is preferred over linear regression in the following situations:\n",
    "\n",
    "Non-linear Relationships: When the relationship between variables is non-linear and cannot be adequately captured by a straight line, polynomial regression can provide a better fit to the data.\n",
    "\n",
    "Curved Patterns: When the relationship between variables exhibits curved or complex patterns, polynomial regression can flexibly model these patterns by fitting higher-degree polynomial terms.\n",
    "\n",
    "Improved Predictive Performance: When linear regression fails to provide satisfactory predictive performance due to the non-linear nature of the data, polynomial regression can lead to improved predictions and better model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fc6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
